{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start spark with 1 worker thread\n",
    "sc = SparkContext(\"local[1]\")\n",
    "sc.setLogLevel(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the machine_events file\n",
    "me_file = sc.textFile(\"./machine_events/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "me_entries = me_file.map(lambda line: line.split(\",\"))\n",
    "me_columns = ['timestamp', 'machineId', 'eventType', 'platformId', 'cpu', 'memory']\n",
    "me_entries = me_entries.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read job_events file\n",
    "je_file = sc.textFile(\"./job_events/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "je_entries = je_file.map(lambda line: line.split(\",\"))\n",
    "je_columns = ['timestamp', 'missing', 'jobId', 'eventType', 'username',\n",
    "              'schedulingClass', 'jobname', 'logicalJobname']\n",
    "je_entries = je_entries.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read task_events file\n",
    "te_file = sc.textFile(\"./task_events/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_entries = te_file.map(lambda line: line.split(\",\"))\n",
    "te_columns = ['timestamp', 'missing', 'jobId', 'taskId', 'machineId',\n",
    "             'eventType', 'username', 'schedulingClass',\n",
    "             'priority', 'rrfCpu', 'rrfRam', 'rrfDiskSpace', 'diffMConstraint']\n",
    "te_entries = te_entries.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read task_usage file\n",
    "tu_file = sc.textFile(\"./task_usage/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tu_entries = tu_file.map(lambda line: line.split(\",\"))\n",
    "tu_entries = tu_entries.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findCol(firstLine, name):\n",
    "\tif name in firstLine:\n",
    "\t\treturn firstLine.index(name)\n",
    "\telse:\n",
    "\t\treturn -1\n",
    "    \n",
    "def getColumn(index,entries):\n",
    "    return entries.map(lambda x: x[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the ID of the job with the highest number of tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind1 = findCol(te_columns, 'jobId')\n",
    "ind2 = findCol(te_columns, 'taskId')\n",
    "\n",
    "job_tasks = te_entries.map(lambda x: (x[ind1],x[ind2])).distinct()\\\n",
    "                      .aggregateByKey(0,lambda x,y: x+1, lambda x,y: x+y)\\\n",
    "                      .sortBy(lambda x: x[1], ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the job with the highest #tasks is 6221861800 with 20010 tasks\n"
     ]
    }
   ],
   "source": [
    "job = job_tasks.take(1)\n",
    "print(f'the job with the highest #tasks is {job[0][0]} with {job[0][1]} tasks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On average, how many tasks get killed or evicted per job?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = findCol(te_columns, 'eventType')\n",
    "ind1 = findCol(te_columns, 'jobId')\n",
    "ind2 = findCol(te_columns, 'taskId')\n",
    "\n",
    "# number of tasks killed or evicted\n",
    "num_of_tasks = te_entries.filter(lambda x: x[ind] in ['2','5'])\\\n",
    "                      .map(lambda x: (x[ind1],x[ind2])).distinct().count()\n",
    "\n",
    "i = findCol(je_columns, 'jobId')\n",
    "\n",
    "# total number of jobs\n",
    "total = getColumn(i,je_entries).distinct().count()\n",
    "average = num_of_tasks / total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9308163265306122"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do tasks with low priority have a higher probability of being evicted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind1 = findCol(te_columns, 'eventType')\n",
    "ind2 = findCol(te_columns, 'priority')\n",
    "\n",
    "evicted_tasks = te_entries.filter(lambda x: x[ind1] in ['1','2'])\\\n",
    "                          .map(lambda x: (x[ind2],((0,1)[x[ind1]=='1'], (0,1)[x[ind1]=='2'] ) ))\\\n",
    "                          .aggregateByKey((0,0),lambda x,y: (x[0]+y[0],x[1]+y[1]), lambda x,y:  (x[0]+y[0],x[1]+y[1]))\n",
    "ratio = evicted_tasks.map(lambda x: (int(x[0]),x[1][1]*100 / x[1][0])).sortBy(lambda x: x[0]).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "priority 0 ratio: 8.207848950612467\n",
      "priority 1 ratio: 0.302185030218503\n",
      "priority 2 ratio: 0.08932486641872249\n",
      "priority 8 ratio: 0.0\n",
      "priority 9 ratio: 0.04471635602349641\n",
      "priority 10 ratio: 0.12004801920768307\n",
      "priority 11 ratio: 0.0\n"
     ]
    }
   ],
   "source": [
    "for t in ratio:\n",
    "    print(f'priority {t[0]} ratio: {t[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see task with very low priority have more or less a higher chance of being evicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### relation between scheduling class of tasks and their priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind1 = findCol(te_columns, 'jobId')\n",
    "ind2 = findCol(te_columns, 'taskId')\n",
    "ind3 = findCol(te_columns, 'priority')\n",
    "ind4 = findCol(te_columns, 'schedulingClass')\n",
    "\n",
    "\n",
    "tasks = te_entries.map(lambda x: ((x[ind1], x[ind2] ), (x[ind4], x[ind3] ) )).distinct()\n",
    "def addLambda(x,y):\n",
    "    x[y]+=1\n",
    "    return x\n",
    "\n",
    "def combine(x,y):\n",
    "    return [x[0]+y[0],x[1]+y[1],x[2]+y[2]]\n",
    "    \n",
    "# a 3 elmts list for each class [(0-3),(4-7),(8-11)] each cell \n",
    "#represents the #task with a priority in the range indicated in the cell\n",
    "tasks = tasks.map(lambda x: (int(x[1][0]), int(x[1][1])//4))\\\n",
    "             .aggregateByKey([0,0,0], addLambda, combine).sortByKey()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [93500, 0, 744]),\n",
       " (1, [12592, 0, 14107]),\n",
       " (2, [23093, 0, 18780]),\n",
       " (3, [2031, 0, 26079])]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each scheduling class there is a list of 3 elements. [#tasks with priority in (0-3),#tasks with priority in (4-7),#tasks with priority in (8-11)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ratio of successfully completed jobs per scheduling class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind1 = findCol(je_columns, 'eventType')\n",
    "ind2 = findCol(je_columns, 'schedulingClass')\n",
    "ind3 = findCol(je_columns, 'jobId')\n",
    "\n",
    "# completed jobs per scheduling class\n",
    "jobs_completed = je_entries.filter(lambda x: x[ind1]=='4').map(lambda x: (int(x[ind2]),1))\n",
    "jobs_completed = jobs_completed.aggregateByKey(0,lambda x,y:x+1, lambda x,y: x+y).collect()\n",
    "\n",
    "# total_jobs per schediling class\n",
    "jobs = je_entries.map(lambda x: (int(x[ind2]),x[ind3])).distinct()\n",
    "jobs = jobs.aggregateByKey(0,lambda x,y:x+1, lambda x,y: x+y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(tup):\n",
    "    di = {}\n",
    "    for a, b in tup: \n",
    "        di.setdefault(a, []).append(b) \n",
    "    return di \n",
    "\n",
    "jobs_compl_dict = convert(jobs_completed)\n",
    "jobs_dict = convert(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the ratio of successful job for scheduling class 0 is: 23.612622415669204\n",
      "the ratio of successful job for scheduling class 1 is: 19.93846153846154\n",
      "the ratio of successful job for scheduling class 2 is: 6.694855532064834\n",
      "the ratio of successful job for scheduling class 3 is: 0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    if i not in jobs_compl_dict:\n",
    "        jobs_compl_dict[i] = [0]\n",
    "    ratio = (0,jobs_compl_dict[i][0])[i in jobs_compl_dict]*100 / jobs_dict[i][0]\n",
    "    print(f'the ratio of successful job for scheduling class {i} is: {ratio}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AS we can see the less latency-sensitive jobs are the higher their chance to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the percentage of computational power lost due to maintenance (a machine wentoffline and reconnected later)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def timeCounter(eventMap):\n",
    "    eventList = eventMap[1]\n",
    "    heapq.heapify(eventList)\n",
    "    heapq.heappop(eventList)  # remove first add\n",
    "    rmTime = False\n",
    "    res = 0\n",
    "    # addTime = 0\n",
    "    # remTime = 0\n",
    "    while len(eventList) > 0:\n",
    "        event = heapq.heappop(eventList)\n",
    "        if event[1] == 1:\n",
    "            rmTime = event[0]\n",
    "        #    remTime +=1\n",
    "        elif rmTime and event[1] == 0:\n",
    "            res += event[0] - rmTime\n",
    "            rmTime = False\n",
    "        #    addTime +=1\n",
    "   # if addTime != 0 or remTime != 0:\n",
    "    #    sys.stdout.write(\"machine: \" + str(eventMap[0]) +\" addTime: \" + str(addTime) + \" resTime: \" + str(remTime)+\"\\n\")\n",
    "    return (eventMap[0], res)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind1 = findCol(me_columns, 'timestamp')\n",
    "ind2 = findCol(me_columns, 'machineId')\n",
    "ind3 = findCol(me_columns, 'eventType')\n",
    "\n",
    "\n",
    "\n",
    "#mappedRDD = rdd.map(mapper_1).cache()\n",
    "mappedRDD = me_entries.map(lambda x: (int(x[ind2]), (int(x[ind1]), int(x[ind3]) ) )).cache()\n",
    "maxTime = mappedRDD.map(lambda t: t[1][0]).max()  # the maxTime traced\n",
    "\n",
    "# RDD for each machine the time it has been added to the system (the minimum time for which there is an event for such machine)\n",
    "machineAddTime = mappedRDD.combineByKey(\n",
    "    lambda t: t[0], lambda x_old, t: min(x_old, t[0]), lambda x0, x1: min(x0, x1))\n",
    "# the sum of the total lifetime of all the machines in microseconds\n",
    "totalMachineTime = machineAddTime.aggregate(\n",
    "    0, lambda x_old, t: x_old + maxTime - t[1], lambda x0, x1: x0+x1)\n",
    "\n",
    "# only keep add and removal events\n",
    "filteredRDD = mappedRDD.filter(lambda t: (t[1][1] == 0 or t[1][1] == 1))\n",
    "# for each machine get a list of event-tuples (time, event-type)\n",
    "aggregatedRDD = filteredRDD.combineByKey(\n",
    "    lambda x: [x], lambda h, x: h + [x], lambda h1, h2: h1+h2)\n",
    "\n",
    "resRDD = aggregatedRDD.map(timeCounter)  # only keep down times\n",
    "# sum downtimes and divide by the total time\n",
    "wasted_time = resRDD.map(lambda x: x[1]).sum()/totalMachineTime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004946197580885792"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wasted_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is the ratio between the wasted time in reboots over the total up-time of machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### average number of tasks per job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind1 = findCol(te_columns, 'jobId')\n",
    "ind2 = findCol(te_columns, 'taskId')\n",
    "\n",
    "countSum = te_entries.map(lambda x: (x[ind1],x[ind2]))\\\n",
    "    .distinct()\\\n",
    "    .aggregateByKey(0, lambda x, y: x+1, lambda x, y: x+y)\\\n",
    "    .map(lambda y: y[1])\\\n",
    "    .aggregate((0, 0), lambda x, y: (x[0]+1, x[1]+y), lambda x, y: (x[0] + y[0], x[1]+y[1]))\n",
    "\n",
    "average_tasks_job = countSum[1]/countSum[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.00429009193054"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_tasks_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.8\n",
    "\n",
    "def adder(d, tuple):\n",
    "    if tuple[1] not in d:\n",
    "        d[tuple[1]] = 1\n",
    "    else:\n",
    "        d[tuple[1]] += 1\n",
    "    return d\n",
    "\n",
    "def merger(d1, d2):\n",
    "    for k in d2:\n",
    "        if k not in d1:\n",
    "            d1[k] = d2[k]\n",
    "        else:\n",
    "            d1[k] += d2[k]\n",
    "    return d1\n",
    "\n",
    "def percentile(tuple1):\n",
    "    di = tuple1[1]\n",
    "    _max = -1\n",
    "    sum = 0\n",
    "    for k in di:\n",
    "        v = di[k]\n",
    "        _max = max(_max, v)\n",
    "        sum += v\n",
    "    return (_max/ sum)\n",
    "\n",
    "def aggregator(tuple1, val):\n",
    "\n",
    "    a = tuple1[0] + 1\n",
    "    b = tuple1[1]\n",
    "    if val > threshold:\n",
    "        b += 1\n",
    "    return (a,b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind1 = findCol(te_columns, 'jobId')\n",
    "ind2 = findCol(te_columns, 'taskId')\n",
    "ind3 = findCol(te_columns, 'machineId')\n",
    "\n",
    "rdd = te_entries.map(lambda x: (int(x[ind1]), (int(x[ind2]), x[ind3] ) ))\n",
    "projectedRDD = rdd.filter(lambda v: v[1][1] != '').distinct()\n",
    "\n",
    "aggregatedRDD = projectedRDD.filter(lambda p: len(p[1])>1).aggregateByKey(dict(), adder, merger)\n",
    "ratio = aggregatedRDD.map(percentile).aggregate((0,0), aggregator, lambda t1,t2: (t1[0]+t2[0], t1[1]+t2[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4879, 3065)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is there a relation between the amount of resource consumed by tasks and their priority?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduleRDD = te_entries.map(lambda lines: ((int(lines[2]), int(lines[3])), int(lines[8])))\n",
    "\n",
    "usageRDD = tu_entries.map(lambda lines: ((int(lines[2]), int(lines[3])), (lines[5], lines[6])))\\\n",
    "                     .filter(lambda x: x[1][0] != '' and x[1][1] != '')\\\n",
    "                     .map(lambda p: ((p[0][0], p[0][1]),(float(p[1][0]),float(p[1][1]))))\\\n",
    "                     .aggregateByKey((0,0,0), lambda x,y: (x[0]+1, x[1]+y[0],x[2]+y[1]),lambda x,y: (x[0]+y[0],x[1]+y[1],x[2]+y[2]))\\\n",
    "                     .map(lambda p: ((p[0][0],p[0][1]), (p[1][1]/p[1][0], p[1][2]/p[1][0])))  # meanCPURate and ramUsage exist\n",
    "    # ((jobIdD, taskID), ((Priority), (meanCPURate,ramUsage)))\n",
    "completeRDD = scheduleRDD.join(usageRDD).cache()\n",
    "sumsCount = completeRDD.aggregate((0, 0, 0, 0),\n",
    "                                  lambda x, y: (x[0]+1, x[1]+y[1][0],\n",
    "                                                x[2]+y[1][1][0], x[3]+y[1][1][1]),\n",
    "                                  lambda x, y: (x[0]+y[0], x[1]+y[1], x[2]+y[2], x[3]+y[3]))\n",
    "avgPriority = sumsCount[1]/sumsCount[0]\n",
    "avgmeanCPURate = sumsCount[2]/sumsCount[0]\n",
    "avgRamUsage = sumsCount[3]/sumsCount[0]\n",
    "\n",
    "\n",
    "diffRDD = completeRDD.map(lambda x: (\n",
    "    x[1][0] - avgPriority, x[1][1][0] - avgmeanCPURate, x[1][1][1] - avgRamUsage))\n",
    "#alfa = diffRDD.take(3)\n",
    "pearsonComponents = diffRDD.aggregate((0., 0., 0., 0., 0.),  # [(xi-x')(yi-y'), (xi-x')(zi-z'), (xi-x')^2, (yi-y')^2, (zi-z')^2] where x is priority, y is cpu, z is ram\n",
    "                                      lambda x, y: (\n",
    "    x[0]+y[0]*y[1], x[1]+y[0]*y[2], x[2]+y[0]*y[0], x[3]+y[1]*y[1], x[4]+y[2]*y[2]),\n",
    "    lambda ya, yb: (ya[0]+yb[0], ya[1]+yb[1], ya[2]+yb[2], ya[3]+yb[3], ya[4]+yb[4]))\n",
    "\n",
    "correlations = pearsonComponents[0]/sqrt(pearsonComponents[2]*pearsonComponents[3]), pearsonComponents[1]/sqrt(pearsonComponents[2]*pearsonComponents[4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.24994329658364695, 0.4687627723345299)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can  we  observe  correlations  between  peaks  of  high  resource  consumption  on  some  ma-chines and task eviction events?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = findCol(te_columns, 'jobId')\n",
    "task_id = findCol(te_columns, 'taskId')\n",
    "machine_id = findCol(te_columns, 'machineId')\n",
    "time_id = findCol(te_columns, 'timestamp')\n",
    "event_id = findCol(te_columns, 'eventType')\n",
    "\n",
    "def mapper_1(line):\n",
    "    machID = -1\n",
    "    if line[machine_id] != '':\n",
    "        machID = int(line[machine_id])\n",
    "    # ((time, machineID), event_type)\n",
    "    return ((int(line[time_id])//300000000, machID), int(line[event_id]))\n",
    "\n",
    "def mapper_2(line):\n",
    "    val = -1\n",
    "    if line[5] != '':\n",
    "        val = float(line[5])\n",
    "    # ((start_time, machine ID), task_usage_rate)\n",
    "    return ((int(line[1])//300000000, int(line[4])), val)\n",
    "def customSum(sum, elem):\n",
    "    if sum>=0 and elem>=0:\n",
    "        return sum+elem\n",
    "    return -1 # -1 elements will be filtered out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evictedCountRDD = te_entries.map(mapper_1).filter(lambda x: x[1] == 2 and x[0][1] >= 0).aggregateByKey(\n",
    "    0, lambda x, y: x+1, lambda x, y: x+y)  # only keep EVICT events and count evicts for time window of 300s\n",
    "\n",
    "machineCPUUsageRDD = tu_entries.map(mapper_2).aggregateByKey(0,customSum, customSum)\\\n",
    "                               .filter(lambda x: x[1] >= 0)  # for pairs (start_time, machine ID) a valid sum of cpu usage (sum of task usages)\n",
    "\n",
    "dataRDD = evictedCountRDD.join(machineCPUUsageRDD).map(\n",
    "    lambda x: (x[1][0], x[1][1])).cache() # result RDD of paired (evictedCount, cpu_usage_rate) for each machine-timeBucket\n",
    "\n",
    "sumsCount= dataRDD.aggregate((0, 0, 0),\n",
    "lambda x, y: (x[0] + 1, x[1]+y[0], x[2]+y[1]),\n",
    "lambda x, y: (x[0] + y[0], x[1] + y[1], x[2]+y[2]))\n",
    "\n",
    "avgEvicted = sumsCount[1]/sumsCount[0]\n",
    "avgPeak = sumsCount[2]/sumsCount[0]\n",
    "\n",
    "pearsonComponents = dataRDD.aggregate((0,0,0), lambda x,y: (x[0]+(y[0]-avgEvicted)*(y[1]-avgPeak), x[1]+(y[0]-avgEvicted)*(y[0]-avgEvicted), x[2]+(y[1]-avgPeak)*(y[1]-avgPeak)),\n",
    "lambda x, y: (x[0] + y[0], x[1] + y[1], x[2]+y[2]))\n",
    "\n",
    "correlations = pearsonComponents[0]/sqrt(pearsonComponents[1]*pearsonComponents[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.060042960762372366"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert RDDs into dataframes\n",
    "te_df = te_entries.toDF(schema=te_columns)\n",
    "je_df = je_entries.toDF(schema=je_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(timestamp='0', missing='2', jobId='3418309', taskId='0', machineId='4155527081', eventType='0', username='70s3v5qRyCO/1PCdI6fVXnrW8FU/w+5CKRSa72xgcIo=', schedulingClass='3', priority='9', rrfCpu='', rrfRam='', rrfDiskSpace='', diffMConstraint=''),\n",
       " Row(timestamp='0', missing='2', jobId='3418309', taskId='1', machineId='329150663', eventType='0', username='70s3v5qRyCO/1PCdI6fVXnrW8FU/w+5CKRSa72xgcIo=', schedulingClass='3', priority='9', rrfCpu='', rrfRam='', rrfDiskSpace='', diffMConstraint=''),\n",
       " Row(timestamp='0', missing='', jobId='3418314', taskId='0', machineId='3938719206', eventType='0', username='70s3v5qRyCO/1PCdI6fVXnrW8FU/w+5CKRSa72xgcIo=', schedulingClass='3', priority='9', rrfCpu='0.125', rrfRam='0.07446', rrfDiskSpace='0.0004244', diffMConstraint='0'),\n",
       " Row(timestamp='0', missing='', jobId='3418314', taskId='1', machineId='351618647', eventType='0', username='70s3v5qRyCO/1PCdI6fVXnrW8FU/w+5CKRSa72xgcIo=', schedulingClass='3', priority='9', rrfCpu='0.125', rrfRam='0.07446', rrfDiskSpace='0.0004244', diffMConstraint='0')]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,udf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Get the ID of the job with the highest number of tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(jobId='6221861800', count=20010)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te_df.select('jobId','taskId').distinct().groupBy('jobId').count().orderBy(col('count').desc()).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On average, how many tasks get killed or evicted per job?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# number of tasks killed or evicted\n",
    "job_tasks = te_df.filter((te_df.eventType=='2') | (te_df.eventType=='5')).select('jobId','taskId').distinct().count()\n",
    "\n",
    "# total number of jobs\n",
    "total = je_df.select('jobId').distinct().count()\n",
    "\n",
    "avg = job_tasks / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9308163265306122"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do tasks with low priority have a higher probability of being evicted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mapper_1(eventType):\n",
    "    evicted = (0,1)[eventType=='2']\n",
    "    return evicted \n",
    "\n",
    "def mapper_2(eventType):\n",
    "    scheduled = (0,1)[eventType=='1']\n",
    "    return scheduled\n",
    "\n",
    "def mapper_3(sCount,eCount):\n",
    "    return eCount*100 / sCount\n",
    "\n",
    "udf1 = udf(mapper_1)\n",
    "udf2 = udf(mapper_2)\n",
    "udf3 = udf(mapper_3)\n",
    "\n",
    "df3 = te_df.filter((te_df.eventType=='1') | (te_df.eventType=='2'))\\\n",
    "           .withColumn('evictedCount', udf1('eventType'))\\\n",
    "           .withColumn('scheduledCount', udf2('eventType'))\\\n",
    "           .withColumn(\"priority\",col(\"priority\").cast('int'))\\\n",
    "           .select('priority','evictedCount','scheduledCount')\\\n",
    "           .groupBy('priority')\\\n",
    "           .agg({'evictedCount': 'sum', 'scheduledCount': 'sum'})\\\n",
    "           .withColumn('ratio', udf3('sum(scheduledCount)','sum(evictedCount)'))\\\n",
    "           .orderBy('priority')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+-------------------+\n",
      "|priority|sum(scheduledCount)|sum(evictedCount)|              ratio|\n",
      "+--------+-------------------+-----------------+-------------------+\n",
      "|       0|            51268.0|           4208.0|  8.207848950612467|\n",
      "|       1|            17208.0|             52.0|  0.302185030218503|\n",
      "|       2|            61573.0|             55.0|0.08932486641872249|\n",
      "|       8|             3371.0|              0.0|                0.0|\n",
      "|       9|            49199.0|             22.0|0.04471635602349641|\n",
      "|      10|              833.0|              1.0|0.12004801920768307|\n",
      "|      11|             6542.0|              0.0|                0.0|\n",
      "+--------+-------------------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what implement 3 of the previous analysis with dataframe. In Dataframe, data are organized into named columns. It is conceptually equal to a table in a relational database. Unlike RDD, Dataframe  will automatically find out the schema of the dataset. For aggreagation it is definetly faster than an RDD and provide an easy API for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
