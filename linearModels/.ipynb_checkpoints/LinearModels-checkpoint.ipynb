{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaline et Régression Logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be interested in the implementation of the perceptron algorithm (Rosenblatt, 68), Adaline (Widrow et Hoff, 60) and Logisitc Regression (Cox, 66) whose pseudo-code are the following:\n",
    "\n",
    "Perceptron:\n",
    "`Input: Train, eta, MaxEp\n",
    "init: w\n",
    "epoch = 0\n",
    "err = 1\n",
    "m = len(Train)\n",
    "while epoque <= MaxEp and err! = 0\n",
    "    err = 0\n",
    "    for i in 1: m\n",
    "        h <- w * x\n",
    "        if (y * h <= 0)\n",
    "            w <- w + eta * y * x\n",
    "            err <- err + 1\n",
    "     epoch <- epoch + 1\n",
    "output: w`\n",
    "\n",
    "Adaline:\n",
    "`input: Train, eta, MaxEp\n",
    "init : w\n",
    "epoque=0\n",
    "err=1\n",
    "m = len(Train)\n",
    "while epoque<=MaxEp and err!=0\n",
    "    err=0\n",
    "    for i in 1:m\n",
    "        h <- w*x\n",
    "        if(y*h<=0)\n",
    "           err <- err+1\n",
    "        w <- w + eta*(y-dp)*x\n",
    "     epoque <- epoque+1\n",
    "output: w`\n",
    "\n",
    "Logistic Regression:\n",
    "`input: Train, eta, MaxEp\n",
    "init : w\n",
    "epoque=0\n",
    "err=1\n",
    "m = len(Train)\n",
    "while epoque<=MaxEp and err!=0\n",
    "    err=0\n",
    "    for i in 1:m\n",
    "        choisir un exemple (x,y) de Train de façon aléatoire\n",
    "        h <- w*x\n",
    "        if(y*h<=0)\n",
    "           err <- err+1\n",
    "        w <- w + eta*y*(1-sigm(y*dp))*x\n",
    "     epoque <- epoque+1\n",
    "output: w`\n",
    "\n",
    "1. Create a list of 4 elements corresponding to the logical AND example called `Train`:\n",
    "$Train=\\{((+1,+1),+1),((-1,+1),-1),((-1,-1),-1),((+1,-1),-1)\\}$\n",
    "\n",
    "Each element of the list is a list which last characteristic is the class of the example and the first characteristics their coordinates.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train=[[+1,+1,+1],[-1,+1,-1],[-1,-1,-1],[+1,-1,-1]] # To be filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Code the Perceptron, Adaline and LR (Logistic regression) programs\n",
    "\n",
    "Hint: You can write a function that calculates the dot product between an example $\\mathbf{x} = (x_1, \\ldots, x_d)$ and the weight vector $\\mathbf{w} = (w_0, w_1, \\ldots, w_d)$: \n",
    "$ h(\\mathbf{x},\\mathbf{w}) = w_0 + \\ sum_ {j = 1} ^ d w_j x_j $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def h(x,w):\n",
    "    # The prediction of the model\n",
    "    Pred=0.0\n",
    "    dotPWX = 0.0\n",
    "    dotPWX=np.dot(w[1:],x)\n",
    "    #for i in range(1,len(w)):\n",
    "        #dotPWX += w[i] * x[i-1]\n",
    "        \n",
    "    Pred = w[0] + dotPWX\n",
    "    return Pred\n",
    "\n",
    "def sigmoid(X):\n",
    "    return 1/(1+np.exp(-X))\n",
    "\n",
    "\n",
    "def Perceptron(Train,eta,MaxEp):\n",
    "    # Perceptron Algorithm \n",
    "    d=len(Train[0])-1\n",
    "    m=len(Train)\n",
    "    W=[0.0 for i in range(d+1)]\n",
    "    err=1\n",
    "    epoque=0\n",
    "    \n",
    "    while epoque<=MaxEp and err!=0:\n",
    "        err=0\n",
    "        for i in range(0,m):\n",
    "            x=Train[i][:d]\n",
    "            Pred = h(x,W)\n",
    "            y=Train[i][d]\n",
    "            if y*Pred<=0.0:\n",
    "                W[0]=W[0]+eta*y\n",
    "                for k in range(d):\n",
    "                    W[k+1]=W[k+1]+eta*y*x[k]\n",
    "                err+=1\n",
    "        epoque+=1\n",
    "    \n",
    "    return W\n",
    "\n",
    "def Adaline(Train,eta,MaxEp):\n",
    "    # Adaline Algorithm \n",
    "    d=len(Train[0])-1\n",
    "    m=len(Train)\n",
    "    W=[0.0 for i in range(d+1)]\n",
    "    err=1\n",
    "    epoque=0\n",
    "    while epoque<=MaxEp and err!=0:\n",
    "        err=0\n",
    "        for i in range(0,m):\n",
    "            x=Train[i][:d]\n",
    "            Pred = h(x,W)\n",
    "            y=Train[i][d]\n",
    "            if y*Pred<=0.0:\n",
    "                err+=1\n",
    "            W[0]=W[0]+eta*(y-Pred)\n",
    "            for k in range(d):\n",
    "                W[k+1]=W[k+1]+eta*(y-Pred)*x[k]\n",
    "        epoque+=1\n",
    "    \n",
    "    \n",
    "    return W\n",
    "\n",
    "def LR(Train,eta,MaxEp):\n",
    "    # Logisitc Regression Algorithm \n",
    "    d=len(Train[0])-1\n",
    "    m=len(Train)\n",
    "    W=[0.0 for i in range(d+1)]\n",
    "    err=1\n",
    "    epoque=0\n",
    "    while epoque<=MaxEp and err!=0:\n",
    "        err=0\n",
    "        for i in range(0,m):\n",
    "            index = np.random.choice([l for l in range(0,m)],1)\n",
    "            x=Train[int(index)][:d]\n",
    "            Pred = h(x,W)\n",
    "            y=Train[int(index)][d]\n",
    "            if y*Pred<=0.0:\n",
    "                err+=1\n",
    "            W[0]=W[0]+eta*(1-sigmoid(y*Pred))\n",
    "            for k in range(d):\n",
    "                W[k+1]=W[k+1]+eta*(1-sigmoid(y*Pred))*x[k]\n",
    "            #W=[W[k+1]+eta*(1-sigmoid(y*Pred))*x[k] for k in range(0,d)]\n",
    "        epoque+=1\n",
    "    return W\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Apply the three learning models on the logical AND, and calculate the model error rate on this basis.\n",
    "\n",
    "Hint: You can write a function that takes a weight vector $\\mathbf{w}$ and an example $(\\mathbf{x},y)$ and calculates the error rate of the model with weight $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. We are now going to focus on the behavior of the three models on http://archive.ics.uci.edu/ml/datasets/connectionist+bench+(sonar,+mines+vs.+rocks), https://archive.ics.uci.edu/ml/datasets/spambase, https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Original%29, https://archive.ics.uci.edu/ml/datasets/Ionosphere. These files are in the current respository with the names `sonar.txt`; `spam.txt`; `wdbc.txt` and `ionoshpere.txt`. We can use the following `ReadCollection` function in order to read the files in the form of the training set that is requested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def Normalize(x):\n",
    "    norm=0.0\n",
    "    for e in x:\n",
    "        norm+=e**2\n",
    "    for i in range(len(x)):\n",
    "        x[i]/=sqrt(norm)\n",
    "    return x\n",
    "\n",
    "# Read the other txt files which have a different format\n",
    "def ReadCollection2(filename):\n",
    "    tag_df=pd.read_table(filename,sep=',',header=None)\n",
    "    Dic = {'g': +1, 'b': -1, 'M': +1, 'R': -1, '1': +1, '0': -1}\n",
    "    X=[]\n",
    "    for e in range(len(tag_df)):\n",
    "        x=list(tag_df.loc[e,:])\n",
    "        cls=str(int(x[-1]) if type(x[-1])== float else x[-1])\n",
    "        #print(cls)\n",
    "        x=x[:len(x)-1]\n",
    "        x=Normalize(x)\n",
    "        x.insert(len(x),Dic[cls])\n",
    "        X.append(x)\n",
    "        \n",
    "    random.shuffle(X)\n",
    "    \n",
    "    return X\n",
    "\n",
    "# Read wdbc.txt file in the Python format of request training set \n",
    "def ReadCollection(filename):\n",
    "    tag_df=pd.read_table(filename,sep=',',header=None)\n",
    "    Dic={'M': -1, 'B': +1}\n",
    "    X=[]\n",
    "    for e in range(len(tag_df)):\n",
    "        x=list(tag_df.loc[e,:])\n",
    "        x.pop(0)\n",
    "        cls=x.pop(0)\n",
    "        x=Normalize(x)\n",
    "        x.insert(len(x),Dic[cls])\n",
    "        X.append(x)\n",
    "\n",
    "    \n",
    "    random.shuffle(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2. Run the three models on these files with $\\eta=0.01$ et $\\eta=0.1$ and `MaxEp=500`.\n",
    " \n",
    " 3. Report in the table below the average of the error rates on the test by repeating each experiment 20 times. \n",
    " \n",
    " <br>\n",
    " <br>\n",
    " \n",
    " \n",
    " <center> $\\eta=0.01$, MaxE$=500$ </center>\n",
    "    \n",
    "    \n",
    "  | Collection | Perceptron | Adaline |    RL    |\n",
    "  |------------|------------|---------|----------|\n",
    "  |   WDBC     |      0.113986      |   0.088111      |    0.377622      |                 \n",
    "  | Ionosphere |   0.1153409         |    0.1011363     |    0.351704      |\n",
    "  |   Sonar    |  0.28365          |   0.25      |   0.4673       |\n",
    "  |   Spam     |      0.23649      |   0.255994      |     0.4563     |\n",
    " \n",
    " <br><br>\n",
    "  \n",
    "  <center> $\\eta=0.1$, MaxEp$=500$ </center>\n",
    "    \n",
    "    \n",
    "  | Collection | Perceptron | Adaline |    RL    |\n",
    "  |------------|------------|---------|----------|\n",
    "  |   WDBC     |  0.101748          |  0.093356       |   0.376923       |                 \n",
    "  | Ionosphere | 0.113636           |   0.10227      |    0.36193      |\n",
    "  |   Sonar    |   0.268269        |  0.264423       |   0.447115       |\n",
    "  |   Spam     |     0.2248479       |  0.27467419       |  0.4305        |\n",
    "  \n",
    "  Hint: you can use the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-6cb496118931>:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-b0dd1040feb6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m#WLA=Adaline(x_train,0.1,500)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m#errA+=EmpiricalRisk(x_test,WLA)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mWLR\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0merrL\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mEmpiricalRisk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mWLR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-52-6cb496118931>\u001b[0m in \u001b[0;36mLR\u001b[1;34m(Train, eta, MaxEp)\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m             \u001b[0mPred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mPred\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-52-6cb496118931>\u001b[0m in \u001b[0;36mh\u001b[1;34m(x, w)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mPred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdotPWX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mdotPWX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;31m#for i in range(1,len(w)):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m#dotPWX += w[i] * x[i-1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def EmpiricalRisk(Test,W):\n",
    "    E=0.0\n",
    "    m=len(Test)\n",
    "    # The empirical error of a model with weight W on a test set of size m\n",
    "    d=len(Test[0])-1\n",
    "    for i in range(m):\n",
    "        x=Test[i][:d]\n",
    "        y=Test[i][d]\n",
    "        Pred = h(x,W)\n",
    "        if y*Pred<=0:\n",
    "            E+=1\n",
    "    \n",
    "    \n",
    "    return E/float(m)\n",
    "\n",
    "X=ReadCollection2(\"spam.txt\")\n",
    "\n",
    "errP=errA=errL=0.0\n",
    "for i in range(1):\n",
    "    x_train ,x_test = train_test_split(X,test_size=0.25)\n",
    "    print(\"iteration= \"+str(i))\n",
    "    WLP=Perceptron(x_train,0.1,500)\n",
    "    errP+=EmpiricalRisk(x_test,WLP)\n",
    "    WLA=Adaline(x_train,0.1,500)\n",
    "    errA+=EmpiricalRisk(x_test,WLA)\n",
    "    WLR=LR(x_train,0.1,500)\n",
    "    errL+=EmpiricalRisk(x_test,WLR)\n",
    "    \n",
    "print(\"Err perceptron=\",errP/float(20),\"Err Adaline=\",errA/float(20),\"Err RL=\",errL/float(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# computation of the error rates with the logical AND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration= 0\n",
      "iteration= 1\n",
      "iteration= 2\n",
      "iteration= 3\n",
      "iteration= 4\n",
      "iteration= 5\n",
      "iteration= 6\n",
      "iteration= 7\n",
      "iteration= 8\n",
      "iteration= 9\n",
      "iteration= 10\n",
      "iteration= 11\n",
      "iteration= 12\n",
      "iteration= 13\n",
      "iteration= 14\n",
      "iteration= 15\n",
      "iteration= 16\n",
      "iteration= 17\n",
      "iteration= 18\n",
      "iteration= 19\n",
      "Err perceptron= 0.55 Err Adaline= 0.7 Err RL= 0.65\n"
     ]
    }
   ],
   "source": [
    "errP=errA=errL=0.0\n",
    "for i in range(20):\n",
    "    x_train ,x_test = train_test_split(Train,test_size=0.25)\n",
    "    print(\"iteration= \"+str(i))\n",
    "    WLP=Perceptron(x_train,0.1,500)\n",
    "    errP+=EmpiricalRisk(x_test,WLP)\n",
    "    WLA=Adaline(x_train,0.1,500)\n",
    "    errA+=EmpiricalRisk(x_test,WLA)\n",
    "    WLR=LR(x_train,0.1,500)\n",
    "    errL+=EmpiricalRisk(x_test,WLR)\n",
    "    \n",
    "print(\"Err perceptron=\",errP/float(20),\"Err Adaline=\",errA/float(20),\"Err RL=\",errL/float(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For eta=0.01 perceptron Err=0.6    adaline Err=0.85   logistic_regression Err=0.55\n",
    "\n",
    "For eta=0.1 perceptron Err=0.55    adaline Err=0.7   logistic_regression Err=0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
